\section{Introduction}
Modern distributed systems usually consist of numerous components and have high complexity. As a result, properly and promptly capturing failures in production environments is perceived as an annoying task ~\cite{dean2009designs, liu2007wids}. While a rich literature has researched the problem of failure detection, it remains challenging.

In fact, many would perceive complete stoppable as the definition of a failure. However, a variety of different kinds of failures exist in production systems. While simple failures could be detected and removed efficiently through extensive testing, others, for example gray failure, remain undiscovered due to their obscure and elusive nature. Gray failure is the type of failure where a system would appear to be alive and functioning but is actually experiencing issues with certain components ~\cite{huang2017gray}. For instance, a background thread responsible for writing data to disk could get stuck while other threads are still functioning. Other examples include non-critical component failure, performance degradation, random packet loss, flaky I/O, memory thrashing and capacity pressure ~\cite{huang2017gray}. Often times, the larger your scale, the more common gray failures become ~\cite{Grayfail28:online}. While some gray failures only have moderate impact on the system, other may result in a catastrophic. An example is ZooKeeper-2201 ~\cite{httpsiss99:online}, a gray failure found in ZooKeeper version 3.4.6 and 3.5.0. It causes the cluster to hang for over 15 minutes even though the leader would still exchange heartbeat messages with its followers ~\cite{beschastnikh2016debugging}. 

Current state-of-the-art approaches struggle at detecting such failures. A recent paper at OSDI '18 ~\cite{huang2018capturing}, titled ``Capturing and Enhancing In Situ System Observability'' by Huang et al., sought to provide a new solution. The observation here is that modern distributed systems generally involve many highly interactive components. When one fails, it is very likely that others would be able to observe it through their interaction with the failed one. So one could totally monitor the status of a component through other components that interact with it. Additionally, different components could report on the status of the target component from different dimensions, including but not limited to memory, cpu and network status of the target. The authors find that \textbf{turning each component into an observer of other components with which it interacts greatly improves a system's observability}. 

Inspired by this finding, the authors design and implement Panorama, a generic failure detection framework that leverages and enhances system observability to detect complex production failures ~\cite{huang2018capturing}. It extracts essential observations from various components and makes inference on the status of a component based on them. Also, given the fact that the majority of components in modern systems are programmed to only handle the errors they encounter when interacting with a failed component but not report them, the authors build a tool that automatically turns components into observers that are capable of reporting observations.

They evaluate Panorama by applying it to detect failures in real-world software like ZooKeeper, Cassandra, HBase and HDFS. They find that it significantly outperforms other state-of-the-art failure detection tools with minor overhead. A follow-up work by the same group of researchers even received a best paper award at NSDI '20 ~\cite{246326}. Attracted by the success of recent work around this idea, we decide to recreate the original work so that we could have a better understanding of the idea and fully grasp of its value. 

We present LittlePanorama, a system that implements core modules of Panorama and provides the same functionality. We build LittlePanorama on top of the classes and types defined in Panorama. We reimplement most modules with much simpler data structures, yet retaining the same program logic and function signatures. One could easily replace Panorama with LittlePanorama, and vice versa. Since we do not have access to the tool that automatically turns components into observers, we do so manually. We demonstrate that LittlePanorama is functioning properly by evaluating it against two artificial crash failures. We then compare LittlePanorama against Panorama using two real-world gray failures. Additionally, we prove that LittlePanorama is able to detect and reflect status changes of a component. Finally, the authors do not disclose any failures used in the paper, nor do they publish anything for reproducibility. We release the two bugs we acquired from the authors and publish all the scripts/software artifacts we build so that other researchers who are also interested Panorama do not have to repeat what we have done.

The rest of this paper is structured as follows. Section 2 introduces the LittlePanorama system. Section 3 discusses our implementation. Section 4 shows how we create failures for testing, while Section 5 evaluates LittlePanorama on those failures. We summarize our thoughts in Section 6, 7 and 8.
